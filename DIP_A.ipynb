{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitheshkamath/DIP-lab_code/blob/main/DIP_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def apply_LoG_filter(image, kernel_size, sigma):\n",
        "    # Apply Gaussian smoothing\n",
        "    blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n",
        "\n",
        "    # Apply Laplacian filter\n",
        "    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n",
        "\n",
        "    # Scale Laplacian response to 0-255 and convert to uint8\n",
        "    scaled_laplacian = cv2.convertScaleAbs(laplacian)\n",
        "\n",
        "    return scaled_laplacian\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread('nat.jpg')\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is None:\n",
        "    print(\"Error: Image not loaded\")\n",
        "    exit(1)\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply LoG filter with kernel size 5 and sigma 1.5\n",
        "result = apply_LoG_filter(gray, 5, 1.5)\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow('LoG Filter Result', result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ugIX9kAKnCLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAPLACE CODE"
      ],
      "metadata": {
        "id": "1JzqH9Slm9PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CANNY EDGE DETECTION"
      ],
      "metadata": {
        "id": "vV0K1to5ne3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread(\"/content/wallpaper1.jpg\", 0)  # Read the image in grayscale\n",
        "\n",
        "# Apply Canny edge detection\n",
        "edges = cv2.Canny(image, 100, 200)  # Adjust the threshold values as needed\n",
        "\n",
        "# Display the original image and the detected edges\n",
        "plt.subplot(121),plt.imshow(image, cmap=\"gray\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.subplot(122),plt.imshow(edges, cmap=\"gray\")\n",
        "plt.title(\"Canny Edges\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2n6WKwNPnlfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DIFFERENCE OF GAUSSIAN"
      ],
      "metadata": {
        "id": "BfT4iMRJoBDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread(\"/content/wallpaper1.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Define the standard deviations for the Gaussian blurring\n",
        "sigma1 = 1.0\n",
        "sigma2 = 2.0\n",
        "\n",
        "# Apply Gaussian blurring\n",
        "blur1 = cv2.GaussianBlur(image, (0, 0), sigma1)\n",
        "blur2 = cv2.GaussianBlur(image, (0, 0), sigma2)\n",
        "\n",
        "# Compute the Difference of Gaussians\n",
        "dog = blur2 - blur1\n",
        "\n",
        "# Thresholding\n",
        "threshold = 30\n",
        "_, thresholded = cv2.threshold(dog, threshold, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Display the original image and the DoG result\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(131)\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(dog, cmap=\"gray\")\n",
        "plt.title(\"Difference of Gaussians\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(thresholded, cmap=\"gray\")\n",
        "plt.title(\"Thresholded DoG\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gbclj5fnoFUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SKELETON"
      ],
      "metadata": {
        "id": "8SpKE206oJPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, morphology\n",
        "\n",
        "# Load the image\n",
        "for i in range(1,7):\n",
        "  j=str(i)\n",
        "  print(\"Skeleton and Image Number \"+j)\n",
        "  image_path = \"/content/\"+j+\".png\"\n",
        "  image = io.imread(image_path, as_gray=True)\n",
        "\n",
        "# Threshold the image (convert it to binary)\n",
        "  threshold = 0.5  # Adjust the threshold value as needed\n",
        "  binary_image = image > threshold\n",
        "\n",
        "# Obtain the skeleton of the binary image\n",
        "  skeleton = morphology.skeletonize(binary_image)\n",
        "\n",
        "# Display the original image and its skeleton side by side\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "  axes[0].imshow(image, cmap='gray')\n",
        "  axes[0].set_title(\"Original Image\")\n",
        "  axes[0].axis('off')\n",
        "  axes[1].imshow(skeleton, cmap='gray')\n",
        "  axes[1].set_title(\"Skeleton\")\n",
        "  axes[1].axis('off')\n",
        "\n",
        "# Show the plots\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3miS1FaboSN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROBOTS OPERATOR"
      ],
      "metadata": {
        "id": "D0tmJrAXoj3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "roberts_cross_v = np.array( [[1, 0 ],\n",
        "                            [0,-1 ]] )\n",
        "\n",
        "roberts_cross_h = np.array( [[ 0, 1 ],\n",
        "                            [ -1, 0 ]] )\n",
        "\n",
        "img = cv2.imread(\"/content/input-300x200.webp\",0).astype('float64')\n",
        "img/=255.0\n",
        "vertical = ndimage.convolve( img, roberts_cross_v )\n",
        "horizontal = ndimage.convolve( img, roberts_cross_h )\n",
        "\n",
        "edged_img = np.sqrt( np.square(horizontal) + np.square(vertical))\n",
        "edged_img*=255\n",
        "cv2.imwrite(\"output.jpg\",edged_img)\n",
        "plt.imshow(edged_img)"
      ],
      "metadata": {
        "id": "_bgn2jTvolyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CORNER DETECTION"
      ],
      "metadata": {
        "id": "Q40x-4hpom0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the required library\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "# read the image\n",
        "img = cv2.imread('corner1.png')\n",
        "plt.imshow(img)\n",
        "# convert image to gray scale image\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# detect corners with the goodFeaturesToTrack function.\n",
        "corners = cv2.goodFeaturesToTrack(gray, 27, 0.01, 10)\n",
        "corners = np.int0(corners)\n",
        "\n",
        "# we iterate through each corner,\n",
        "# making a circle at each point that we think is a corner.\n",
        "for i in corners:\n",
        "    x, y = i.ravel()\n",
        "    cv2.circle(img, (x, y), 3, 255, -1)\n",
        "\n",
        "plt.imshow(img), plt.show()"
      ],
      "metadata": {
        "id": "bAhkG5IZotUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HARRIS METHOD OF CORNER DETECTION"
      ],
      "metadata": {
        "id": "axTKWH5Lo2M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to illustrate\n",
        "# corner detection with\n",
        "# Harris Corner Detection Method\n",
        "\n",
        "# organizing imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# path to input image specified and\n",
        "# image is loaded with imread command\n",
        "image = cv2.imread('/content/helloworld.png')\n",
        "\n",
        "# convert the input image into\n",
        "# grayscale color space\n",
        "operatedImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# modify the data type\n",
        "# setting to 32-bit floating point\n",
        "operatedImage = np.float32(operatedImage)\n",
        "\n",
        "# apply the cv2.cornerHarris method\n",
        "# to detect the corners with appropriate\n",
        "# values as input parameters\n",
        "dest = cv2.cornerHarris(operatedImage, 2, 5, 0.07)\n",
        "\n",
        "# Results are marked through the dilated corners\n",
        "dest = cv2.dilate(dest, None)\n",
        "\n",
        "# Reverting back to the original image,\n",
        "# with optimal threshold value\n",
        "image[dest > 0.01 * dest.max()]=[0, 0, 255]\n",
        "\n",
        "# the window showing output image with corners\n",
        "cv2_imshow(image)\n",
        "\n",
        "# De-allocate any associated memory usage\n",
        "if cv2.waitKey(0) & 0xff == 27:\n",
        "\tcv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "o0lcIQtXo6Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOBEL OPERATOR\n"
      ],
      "metadata": {
        "id": "UuIBrWq5pJyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the original image\n",
        "img = cv2.imread('nature.jpg')\n",
        "# converting because opencv uses BGR as default\n",
        "RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(RGB_img)\n",
        "# converting to gray scale\n",
        "gray = cv2.cvtColor(RGB_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# remove noise\n",
        "img = cv2.GaussianBlur(gray,(3,3),0)\n",
        "\n",
        "# convolute with sobel kernels\n",
        "sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  # x\n",
        "sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  # y\n",
        "\n",
        "#Plotting images\n",
        "plt.imshow(sobelx)\n",
        "plt.title(\"Sobel-x edge detection\")\n",
        "\n",
        "plt.imshow(sobely)\n",
        "plt.title(\"Sobel-y edge detection\")"
      ],
      "metadata": {
        "id": "_9ZjlJhjpLrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHAIN CODE"
      ],
      "metadata": {
        "id": "_1jG-Id7pPgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This line imports the NumPy library, which provides support for arrays and mathematical operations.\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to generate Freeman chain code.This line defines a function named generate_chain_code that takes a boundary as input.\n",
        "def generate_chain_code(boundary):\n",
        "    # Define the direction codes.The direction vectors are represented as tuples, and the codes range from 0 to 7.\n",
        "    direction_codes = {\n",
        "        (0, 1): 0,\n",
        "        (-1, 1): 1,\n",
        "        (-1, 0): 2,\n",
        "        (-1, -1): 3,\n",
        "        (0, -1): 4,\n",
        "        (1, -1): 5,\n",
        "        (1, 0): 6,\n",
        "        (1, 1): 7,\n",
        "    }\n",
        "\n",
        "    # This line initializes an empty list named chain_code to store the chain code sequence.\n",
        "    chain_code = []\n",
        "\n",
        "    # This line starts a loop that iterates over the indices of the boundary list, excluding the last index.\n",
        "    for i in range(len(boundary) - 1):\n",
        "        # hese lines calculate the differences in x and y coordinates between the current point and the next point in the boundary list.\n",
        "        dx = boundary[i + 1][0] - boundary[i][0]\n",
        "        dy = boundary[i + 1][1] - boundary[i][1]\n",
        "\n",
        "        # Assign the direction code to the chain code sequence\n",
        "        try:\n",
        "            direction = direction_codes[(dx, dy)]\n",
        "        except KeyError:\n",
        "            direction = 8  # Default direction code for unrecognized movements\n",
        "        chain_code.append(direction)\n",
        "\n",
        "    return chain_code\n",
        "\n",
        "\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Apply thresholding to create a binary image\n",
        "_, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Find contours in the binary image\n",
        "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Select the largest contour\n",
        "largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "# Convert contour to list of points\n",
        "boundary = largest_contour.reshape(-1, 2).tolist()\n",
        "\n",
        "# Generate Freeman chain code\n",
        "chain_code = generate_chain_code(boundary)\n",
        "\n",
        "# Print the chain code\n",
        "print(chain_code)"
      ],
      "metadata": {
        "id": "F9ekCTF-pW_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEMPELATE MATCHING"
      ],
      "metadata": {
        "id": "SiiA1sEapX70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "img_rgb =cv2.imread(\"bub.png\")\n",
        "img_gray=cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
        "template=cv2.imread(\"bub.png\",0)\n",
        "h,w=template.shape[::]\n",
        "res=cv2.matchTemplate(img_gray, template, cv2.TM_CCOEFF_NORMED)\n",
        "plt.imshow(res, cmap='gray')\n",
        "threshold=0.8\n",
        "loc=np.where(res>=threshold)\n",
        "for pt in zip(*loc[::-1]):\n",
        "    cv2.rectangle(img_rgb, pt,(pt[0]+ w,pt[1] +h),(0,0,255),2)\n",
        "    cv2.imshow(\"Matched Image\",img_rgb)\n",
        "    cv2.waitKey()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "78sDwLn7qArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE SEGMENTATION"
      ],
      "metadata": {
        "id": "HyNJY8unqBoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "#loading original image\n",
        "img = cv2.imread('/content/img.jpg')\n",
        "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(img,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()\n",
        "#converting to gray scale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(gray,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"GrayScale Image\")\n",
        "plt.show()\n",
        "#converting to a binary inverted image\n",
        "ret, thresh = cv2.threshold(gray, 0, 255,cv2.THRESH_BINARY_INV +cv2.THRESH_OTSU)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(thresh,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Threshold Image\")\n",
        "plt.show()\n",
        "#segmenting the image\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE,kernel, iterations = 15)\n",
        "bg = cv2.dilate(closing, kernel, iterations = 1)\n",
        "dist_transform = cv2.distanceTransform(closing, cv2.DIST_L2, 0)\n",
        "ret, fg = cv2.threshold(dist_transform, 0.02*dist_transform.max(), 255, 0)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(fg,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Segmented Image\")\n",
        "plt.show()\n",
        "#final output\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.axis('off')\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(img,cmap=\"gray\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(gray,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"GrayScale Image\")\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(thresh,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Threshold Image\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(fg,cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Segmented Image\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8jhmIw0qMcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDGE RELAXATION"
      ],
      "metadata": {
        "id": "l_uvc1fhqs0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.ndimage.filters import convolve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the edge relaxation kernel\n",
        "kernel = np.array([[0, 1, 0],\n",
        "                   [1, 4, 1],\n",
        "                   [0, 1, 0]])\n",
        "\n",
        "# Define the edge relaxation threshold\n",
        "threshold = 50\n",
        "\n",
        "\n",
        "def edge_relaxation(image):\n",
        "    # Convert image to grayscale if needed\n",
        "    if len(image.shape) > 2:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Convert image to float for calculations\n",
        "    image = image.astype(np.float32)\n",
        "\n",
        "    # Normalize image to range [0, 1]\n",
        "    image /= 255.0\n",
        "\n",
        "    # Perform edge relaxation iterations\n",
        "    while True:\n",
        "        # Apply the edge relaxation kernel\n",
        "        smoothed_image = convolve(image, kernel, mode='constant')\n",
        "\n",
        "        # Find the difference between the original and smoothed images\n",
        "        diff = np.abs(image - smoothed_image)\n",
        "\n",
        "        # Update the image with pixels that have a difference above the threshold\n",
        "        image = np.where(diff > threshold, smoothed_image, image)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.max(diff) <= threshold:\n",
        "            break\n",
        "\n",
        "    # Convert image back to the range [0, 255]\n",
        "    image *= 255.0\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Apply edge relaxation\n",
        "relaxed_image = edge_relaxation(image)\n",
        "\n",
        "# Convert images to RGB for compatibility with matplotlib\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "relaxed_image_rgb = cv2.cvtColor(relaxed_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the original and relaxed images side by side using matplotlib\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(image_rgb)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(relaxed_image_rgb)\n",
        "axes[1].set_title('Relaxed Image')\n",
        "axes[1].axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LE6MMdbOq710"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPH THEORITIC ALGORITHM"
      ],
      "metadata": {
        "id": "0JBLvIa5q8mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define 8-connectivity neighbors\n",
        "neighbors = [(0, 1), (1, 0), (0, -1), (-1, 0), (1, 1), (-1, -1), (1, -1), (-1, 1)]\n",
        "\n",
        "def heuristic(node, goal):\n",
        "    # Calculate Euclidean distance as the heuristic\n",
        "    return np.linalg.norm(np.array(node) - np.array(goal))\n",
        "\n",
        "def edge_linking(image):\n",
        "    # Convert image to binary\n",
        "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Get image dimensions\n",
        "    height, width = binary_image.shape[:2]\n",
        "\n",
        "    # Create a visited matrix to keep track of visited pixels\n",
        "    visited = np.zeros((height, width), dtype=bool)\n",
        "\n",
        "    # Create a list to store object boundaries\n",
        "    object_boundaries = []\n",
        "\n",
        "    # Iterate over each pixel in the image\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            # Check if the pixel is an unvisited edge pixel\n",
        "            if binary_image[y, x] == 255 and not visited[y, x]:\n",
        "                # Start a new object boundary\n",
        "                object_boundary = []\n",
        "\n",
        "                # Perform A* algorithm to find connected edge pixels\n",
        "                open_list = []\n",
        "                heapq.heappush(open_list, (0, (y, x)))\n",
        "                while open_list:\n",
        "                    _, (cy, cx) = heapq.heappop(open_list)\n",
        "                    visited[cy, cx] = True\n",
        "                    object_boundary.append((cx, cy))\n",
        "\n",
        "                    # Check neighboring pixels\n",
        "                    for dy, dx in neighbors:\n",
        "                        ny, nx = cy + dy, cx + dx\n",
        "                        if 0 <= ny < height and 0 <= nx < width and binary_image[ny, nx] == 255 and not visited[ny, nx]:\n",
        "                            g = 1  # Cost of moving to a neighboring pixel\n",
        "                            h = heuristic((ny, nx), (y, x))\n",
        "                            f = g + h\n",
        "                            heapq.heappush(open_list, (f, (ny, nx)))\n",
        "                            visited[ny, nx] = True\n",
        "\n",
        "                # Add the object boundary to the list\n",
        "                object_boundaries.append(object_boundary)\n",
        "\n",
        "    return object_boundaries\n",
        "\n",
        "# Read the image\n",
        "image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Perform edge linking with A* algorithm\n",
        "object_boundaries = edge_linking(image)\n",
        "\n",
        "# Create a color image for visualization\n",
        "image_with_boundaries = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "# Draw the object boundaries on the image\n",
        "for object_boundary in object_boundaries:\n",
        "    for x, y in object_boundary:\n",
        "        image_with_boundaries[y, x] = (0, 0, 255)  # Draw boundary pixels in red\n",
        "\n",
        "# Convert images to RGB for compatibility with matplotlib\n",
        "image_rgb = cv2.cvtColor(image_with_boundaries, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the original image and image with object boundaries side by side using matplotlib\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(image, cmap='gray')\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(image_rgb)\n",
        "axes[1].set_title('Image with Object Boundaries (A*)')\n",
        "axes[1].axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KZ7Xz6sRrDeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HIT & MISS TRANSFORM"
      ],
      "metadata": {
        "id": "ExIeulolrEjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import mahotas as mh\n",
        "import numpy as np\n",
        "from pylab import imshow, show\n",
        "\n",
        "# creating region\n",
        "# numpy.ndarray\n",
        "regions = np.zeros((10, 10), bool)\n",
        "\n",
        "# setting 1 value to the region\n",
        "regions[1, :2] = 1\n",
        "regions[5:8, 6: 8] = 1\n",
        "regions[8, 0] = 1\n",
        "\n",
        "# showing the image with interpolation = 'nearest'\n",
        "print(\"\\noriginal Image\")\n",
        "imshow(regions, interpolation ='nearest')\n",
        "show()\n",
        "\n",
        "# template for hit miss\n",
        "template = np.array([\n",
        "\t\t\t[0, 1, 1],\n",
        "\t\t\t[0, 1, 1],\n",
        "\t\t\t[0, 1, 1]])\n",
        "\n",
        "# hit miss transform\n",
        "img = mh.hitmiss(regions, template)\n",
        "\n",
        "# showing image\n",
        "print(\"\\nImage after hit miss transform\")\n",
        "imshow(img)\n",
        "show()\n"
      ],
      "metadata": {
        "id": "EPaHeoWlrkCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADAPTIVE THRESHOLDING"
      ],
      "metadata": {
        "id": "fQTDL6Brrl1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Open the input image using Pillow\n",
        "image1 = Image.open('download.jpeg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "img = image1.convert('L')\n",
        "\n",
        "# Convert the Pillow image to a NumPy array\n",
        "img_array = np.array(img)\n",
        "\n",
        "# Applying different thresholding techniques\n",
        "thresh1 = cv2.adaptiveThreshold(img_array, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                               cv2.THRESH_BINARY, 199, 5)\n",
        "\n",
        "thresh2 = cv2.adaptiveThreshold(img_array, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                               cv2.THRESH_BINARY, 199, 5)\n",
        "\n",
        "# Convert the NumPy array back to a Pillow image\n",
        "result1 = Image.fromarray(thresh1)\n",
        "result2 = Image.fromarray(thresh2)\n",
        "\n",
        "# Display the images\n",
        "image1.show(title='original image')\n",
        "result1.show(title='Adaptive Mean')\n",
        "result2.show(title='Adaptive Gaussian')"
      ],
      "metadata": {
        "id": "FHKA5zr7rrsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVEX HULL\n"
      ],
      "metadata": {
        "id": "ixnkoqporsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
        "import numpy as np\n",
        "rng = np.random.default_rng()\n",
        "points = rng.random((30, 2))   # 30 random points in 2-D\n",
        "hull = ConvexHull(points)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(points[:,0], points[:,1], 'o')\n",
        "for simplex in hull.simplices:\n",
        "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
        "plt.plot(points[hull.vertices,0], points[hull.vertices,1], 'r--', lw=2)\n",
        "plt.plot(points[hull.vertices[0],0], points[hull.vertices[0],1], 'ro')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rZ1rlU9dsPPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OTSU CODE"
      ],
      "metadata": {
        "id": "xG3JsLvZsQLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to illustrate\n",
        "# Otsu thresholding type on an image\n",
        "\n",
        "# organizing imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# path to input image is specified and\n",
        "# image is loaded with imread command\n",
        "image1 = cv2.imread('/content/tswift.jpg')\n",
        "cv2_imshow(image1)\n",
        "\n",
        "# cv2.cvtColor is applied over the\n",
        "# image input with applied parameters\n",
        "# to convert the image in grayscale\n",
        "img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# applying Otsu thresholding\n",
        "# as an extra flag in binary\n",
        "# thresholding\n",
        "ret, thresh1 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY +\n",
        "                                            cv2.THRESH_OTSU)\n",
        "\n",
        "# the window showing output image\n",
        "# with the corresponding thresholding\n",
        "# techniques applied to the input image\n",
        "cv2_imshow( thresh1)\n",
        "\n",
        "# De-allocate any associated memory usage\n",
        "if cv2.waitKey(0) & 0xff == 27:\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "5QP0AhGlsRS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRINCIPLES OF TRESHOLDING"
      ],
      "metadata": {
        "id": "nlzSYSL9sVsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('vehicles.mp4')\n",
        "\n",
        "#tracker = cv2.legacy.TrackerMOSSE_create()\n",
        "\n",
        "\n",
        "object_detector = cv2.createBackgroundSubtractorMOG2(history=100,varThreshold=40)#higher the threshold less is the false detection\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    height, width, _ = frame.shape\n",
        "    #print(height,width)\n",
        "    newframe = frame\n",
        "    roi = newframe[170: 300,300:400]\n",
        "\n",
        "    mask = object_detector.apply(roi) #frame to roi\n",
        "\n",
        "    _, mask = cv2.threshold(mask,254,255,cv2.THRESH_BINARY)\n",
        "\n",
        "\n",
        "    #detections = []\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if area >100 :\n",
        "          #cv2.drawContours(roi,[cnt],-1,(0,255,0),2) #here we changed from frame to roi\n",
        "          x, y, w, h = cv2.boundingRect(cnt)\n",
        "          cv2.rectangle(roi, (x,y), (x+w,y+h), (255,0,0),2)\n",
        "          #detections.append([x,y,w,h])\n",
        "\n",
        "   # print(detections)\n",
        "   #  cv2.imshow(\"roi\",roi)\n",
        "    cv2.imshow(\"Original\", frame)\n",
        "    cv2.imshow(\"Detected\", newframe)\n",
        "    # cv2.imshow(\"mask\", mask)\n",
        "\n",
        "    key = cv2.waitKey(30)\n",
        "\n",
        "    if key == 27:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cap.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "yfRt4hMSst-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PATTERN FIT ALGORITHM"
      ],
      "metadata": {
        "id": "QD9M4HLisu_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def pattern_fit(image, template):\n",
        "    result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
        "    _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
        "    return max_val, max_loc\n",
        "\n",
        "# Load the main image\n",
        "image = cv2.imread('image4.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Load the template image\n",
        "template = cv2.imread('image5.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Find the pattern in the image\n",
        "similarity, location = pattern_fit(image, template)\n",
        "\n",
        "# Draw a rectangle around the matched pattern\n",
        "h, w = template.shape\n",
        "top_left = location\n",
        "bottom_right = (top_left[0] + w, top_left[1] + h)\n",
        "cv2.rectangle(image, top_left, bottom_right, 255, 2)\n",
        "\n",
        "# Display the result\n",
        "cv2_imshow( image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "3kouNcNwtamg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLOBAL THREHSOLDING ALGORITHM"
      ],
      "metadata": {
        "id": "69uEUaCttvR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to illustrate\n",
        "# simple thresholding type on an image\n",
        "\n",
        "# organizing imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# path to input image is specified and\n",
        "# image is loaded with imread command\n",
        "image1 = cv2.imread('/content/v.jpg')\n",
        "\n",
        "# cv2.cvtColor is applied over the\n",
        "# image input with applied parameters\n",
        "# to convert the image in grayscale\n",
        "img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# applying different thresholding\n",
        "# techniques on the input image\n",
        "# all pixels value above 120 will\n",
        "# be set to 255\n",
        "ret, thresh1 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY)\n",
        "ret, thresh2 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY_INV)\n",
        "ret, thresh3 = cv2.threshold(img, 120, 255, cv2.THRESH_TRUNC)\n",
        "ret, thresh4 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO)\n",
        "ret, thresh5 = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO_INV)\n",
        "\n",
        "# the window showing output images\n",
        "# with the corresponding thresholding\n",
        "# techniques applied to the input images\n",
        "cv2_imshow(image1)\n",
        "cv2_imshow( thresh1)\n",
        "cv2_imshow( thresh2)\n",
        "cv2_imshow( thresh3)\n",
        "cv2_imshow(thresh4)\n",
        "cv2_imshow( thresh5)\n",
        "\n",
        "# De-allocate any associated memory usage\n",
        "if cv2.waitKey(0) & 0xff == 27:\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "klv9eDrSt0CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BACKGROUNG REPRESENTATION"
      ],
      "metadata": {
        "id": "YVxtHw1zt04G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('img.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Canny edge detection\n",
        "edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "# Perform morphological closing to connect broken edges\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "closed_edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# Find contours of the closed edges\n",
        "contours, _ = cv2.findContours(closed_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Sort the contours by area in descending order\n",
        "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
        "\n",
        "# Extract the largest contour\n",
        "largest_contour = contours[0]\n",
        "\n",
        "# Create a blank image for boundary representation\n",
        "boundary_image = np.zeros_like(image, dtype=np.uint8)\n",
        "\n",
        "# Draw the boundary on the image\n",
        "cv2.drawContours(boundary_image, [largest_contour], 0, (0, 255, 0), thickness=2)\n",
        "\n",
        "# Display the original image and enhanced boundary representation\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(boundary_image)"
      ],
      "metadata": {
        "id": "58ZURDjnuK_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SKELETANIZATION"
      ],
      "metadata": {
        "id": "xjZt89cKuW8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage import morphology, io\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Read the input image\n",
        "image = io.imread(\"image.jpg\", as_gray=True)\n",
        "\n",
        "# Perform binary thresholding\n",
        "threshold = 0.5\n",
        "binary_image = image > threshold\n",
        "\n",
        "# Apply skeletonization\n",
        "skeleton_image = morphology.skeletonize(binary_image)\n",
        "\n",
        "# Display the skeleton image\n",
        "io.imshow(skeleton_image)\n",
        "io.show()"
      ],
      "metadata": {
        "id": "w2u2eigTuXPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE BACKGROUND FORMAT REPRESENTATION"
      ],
      "metadata": {
        "id": "YihspANCumJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('img.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Initialize the feature detector\n",
        "detector = cv2.SIFT_create()\n",
        "\n",
        "# Detect keypoints and compute descriptors\n",
        "keypoints, descriptors = detector.detectAndCompute(gray, None)\n",
        "\n",
        "print(\"Number of keypoints:\", len(keypoints))\n",
        "\n",
        "# Extract keypoint coordinates\n",
        "keypoint_coords = np.float32([kp.pt for kp in keypoints])\n",
        "\n",
        "# Compute the convex hull of keypoint coordinates\n",
        "hull = cv2.convexHull(keypoint_coords)\n",
        "\n",
        "print(\"Hull:\", hull)\n",
        "print(\"Number of points in hull:\", len(hull))\n",
        "\n",
        "# Create a blank image for boundary representation\n",
        "boundary_image = np.zeros_like(image, dtype=np.uint8)\n",
        "\n",
        "# Draw lines between keypoints in the hull\n",
        "for i in range(len(hull) - 1):\n",
        "    pt1 = tuple(map(int, hull[i][0]))\n",
        "    pt2 = tuple(map(int, hull[i + 1][0]))\n",
        "    cv2.line(boundary_image, pt1, pt2, (0, 255, 0), thickness=1)\n",
        "\n",
        "# Connect the first and last keypoints\n",
        "pt1 = tuple(map(int, hull[0][0]))\n",
        "pt2 = tuple(map(int, hull[-1][0]))\n",
        "cv2.line(boundary_image, pt1, pt2, (0, 255, 0), thickness=1)\n",
        "\n",
        "# Display the original image and boundary representation\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(boundary_image)"
      ],
      "metadata": {
        "id": "aX4PzRWvuq1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POLYGONAL APPROXIMATION"
      ],
      "metadata": {
        "id": "-_wnnl2_u4L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_perimeter_polygon(points):\n",
        "    if len(points) < 3:\n",
        "        return None\n",
        "    hull = ConvexHull(points)\n",
        "    vertices = hull.vertices\n",
        "    return [points[vertex] for vertex in vertices]\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('cr7.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Threshold the grayscale image\n",
        "_, threshold = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Find contours in the thresholded image\n",
        "contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Iterate over the contours and approximate polygons\n",
        "polygons = []\n",
        "for contour in contours:\n",
        "    contour = np.squeeze(contour)\n",
        "    polygon = minimum_perimeter_polygon(contour)\n",
        "    if polygon is not None:\n",
        "        polygons.append(np.array(polygon))\n",
        "\n",
        "# Create a blank image for drawing the contours\n",
        "output = np.zeros_like(image)\n",
        "\n",
        "# Draw the polygons on the image\n",
        "cv2.drawContours(output, polygons, -1, (0, 255, 0), 2)\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow(\"Original Image\",image)\n",
        "cv2.imshow('Polygonal Approximation', output)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "cMSDyyVXvt4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HAUGH TRANSFORM"
      ],
      "metadata": {
        "id": "VQI2KPtFv6uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('car.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "\n",
        "# Apply edge detection\n",
        "edges = cv2.Canny(image, 50, 150, apertureSize=3)\n",
        "cv2.imshow('edge detection',image)\n",
        "\n",
        "# Perform Hough Transform to detect lines\n",
        "lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
        "\n",
        "# Draw detected lines on the image\n",
        "if lines is not None:\n",
        "    for rho, theta in lines[0]:\n",
        "        a = np.cos(theta)\n",
        "        b = np.sin(theta)\n",
        "        x0 = a*rho\n",
        "        y0 = b*rho\n",
        "        x1 = int(x0 + 1000*(-b))\n",
        "        y1 = int(y0 + 1000*(a))\n",
        "        x2 = int(x0 - 1000*(-b))\n",
        "        y2 = int(y0 - 1000*(a))\n",
        "\n",
        "        cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow('Hough Transform', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "0UeC0NLuv8mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOUNDARY EXTRACTION"
      ],
      "metadata": {
        "id": "xta7rSVZwubh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "#\n",
        "# # Generate a synthetic binary image\n",
        "# binary_image = np.zeros((100, 100), dtype=np.uint8)\n",
        "# binary_image[40:60, 40:60] = 255 # Create a square region as foreground\n",
        "#\n",
        "# # Save the binary image\n",
        "# cv2.imwrite('binary_image.png',binary_image)\n",
        "# Read the binary image\n",
        "image = cv2.imread('binary_img.png', 0)\n",
        "# Apply morphological operations for boundary extraction\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "dilated = cv2.dilate(image, kernel, iterations=1)\n",
        "eroded = cv2.erode(image, kernel, iterations=1)\n",
        "boundary = dilated - eroded\n",
        "# Display the result\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Boundary', boundary)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "pGjiymv6wwEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HOLE FILLING"
      ],
      "metadata": {
        "id": "OApEqFvCwwx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# Read the binary image\n",
        "image = cv2.imread('hole.png', 0)\n",
        "# Create a copy of the binary image\n",
        "filled_image = image.copy()\n",
        "# Find contours of the inverted image\n",
        "contours, hierarchy = cv2.findContours(filled_image, cv2.RETR_EXTERNAL,\n",
        "cv2.CHAIN_APPROX_SIMPLE)\n",
        "# Fill the contours (holes) with white\n",
        "cv2.drawContours(filled_image, contours, -1, 255, cv2.FILLED)\n",
        "# Display the result\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Filled Image', filled_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "lnm7g0-ew5Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXTRACTION OF CONNECTED COMPONENTS"
      ],
      "metadata": {
        "id": "mSt7qsHow6Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "def region_growing(image, seed, threshold):\n",
        " height, width = image.shape\n",
        " visited = np.zeros_like(image)\n",
        " region = np.zeros_like(image)\n",
        " queue = [seed]\n",
        " while queue:\n",
        " current_pixel = queue.pop(0)\n",
        " x, y = current_pixel\n",
        " if visited[x, y] == 0 and abs(float(image[x, y]) - float(image[seed])) <= threshold:\n",
        " visited[x, y] = 1\n",
        " region[x, y] = 255\n",
        " if x > 0:\n",
        " queue.append((x - 1, y))\n",
        " if x < height - 1:\n",
        " queue.append((x + 1, y))\n",
        " if y > 0:\n",
        " queue.append((x, y - 1))\n",
        " if y < width - 1:\n",
        " queue.append((x, y + 1))\n",
        " return region\n",
        "# Load the image\n",
        "image_path = 'images.jpg'\n",
        "image = Image.open(image_path).convert('L')\n",
        "image_array = np.array(image, dtype=np.float32)\n",
        "# Define the seed pixel and threshold\n",
        "seed_pixel = (50,50)\n",
        "threshold = 100\n",
        "# Apply region growing algorithm\n",
        "result = region_growing(image_array, seed_pixel, threshold)\n",
        "# Save the result as an image\n",
        "result_image = Image.fromarray(result.astype(np.uint8))\n",
        "result_image.save('result.jpg')\n",
        "image.show()\n",
        "result_image.show()"
      ],
      "metadata": {
        "id": "Ne1UoqtXw_5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HARR TRANSFORM"
      ],
      "metadata": {
        "id": "lko1lPFixref"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def haar_transform_2D(image):\n",
        "    # Convert image to float32\n",
        "    image = np.float32(image)\n",
        "\n",
        "    # Apply Haar transform to rows\n",
        "    rows, cols = image.shape\n",
        "    transformed_rows = np.zeros((rows, cols), dtype=np.float32)\n",
        "    for i in range(rows):\n",
        "        transformed_rows[i, :] = haar_transform(image[i, :])\n",
        "\n",
        "    # Apply Haar transform to columns\n",
        "    transformed_image = np.zeros((rows, cols), dtype=np.float32)\n",
        "    for j in range(cols):\n",
        "        transformed_image[:, j] = haar_transform(transformed_rows[:, j])\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "def inverse_haar_transform_2D(transformed_image):\n",
        "    # Apply inverse Haar transform to columns\n",
        "    rows, cols = transformed_image.shape\n",
        "    inverse_transformed_rows = np.zeros((rows, cols), dtype=np.float32)\n",
        "    for j in range(cols):\n",
        "        inverse_transformed_rows[:, j] = inverse_haar_transform(transformed_image[:, j])\n",
        "\n",
        "    # Apply inverse Haar transform to rows\n",
        "    reconstructed_image = np.zeros((rows, cols), dtype=np.float32)\n",
        "    for i in range(rows):\n",
        "        reconstructed_image[i, :] = inverse_haar_transform(inverse_transformed_rows[i, :])\n",
        "\n",
        "    # Convert image back to uint8\n",
        "    reconstructed_image = np.uint8(reconstructed_image)\n",
        "\n",
        "    return reconstructed_image\n"
      ],
      "metadata": {
        "id": "_KMWlQhTxs48"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}